import os
import streamlit as st
import tensorflow as tf
import numpy as np
import librosa
import sounddevice as sd
import wavio
from tensorflow.keras.models import load_model

os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"  # Disable oneDNN logs
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '4'  # Suppress CPU warnings
tf.get_logger().setLevel("ERROR")  # Suppress general TensorFlow logs

# ‚úÖ Disable oneDNN warning
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

# ‚úÖ Suppress TensorFlow deprecation warnings
tf.get_logger().setLevel("ERROR")

# ‚úÖ Set Page Config - ONLY ONCE at the start!
st.set_page_config(
    page_title="Speech Emotion Recognition",
    page_icon="üé§",
    layout="centered",
    initial_sidebar_state="expanded"
)

# ‚úÖ Use a common directory for both uploaded and recorded audio
AUDIO_SAVE_PATH = "audio_files"
if not os.path.exists(AUDIO_SAVE_PATH):
    os.makedirs(AUDIO_SAVE_PATH)

# ‚úÖ Load Model
model_path = r"C:\Users\PRIYA GUPTA\OneDrive\Desktop\OneDrive\Desktop\SAP\TESS Toronto emotional speech set data\speech_Emotion_Recognition.keras"

try:
    model = load_model(model_path, compile=False)
    model.compile(optimizer="rmsprop", loss="categorical_crossentropy")  # Recompile with default optimizer
    st.success("‚úÖ Model loaded successfully!")
except Exception as e:
    st.error(f"‚ùå Error loading model: {e}")
    model = None  # Prevents errors later


# ‚úÖ Feature Extraction Function
def extract_features(audio_file):
    try:
        y, sr = librosa.load(audio_file, sr=22050)

        # Ensure audio is not empty
        if len(y) == 0:
            st.error("‚ùå Extracted audio is empty. Try recording again.")
            return None

        # Extract MFCC features
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)

        # Ensure features are extracted
        if mfccs.shape[1] == 0:
            st.error("‚ùå No features extracted from audio!")
            return None

        return np.mean(mfccs.T, axis=0)
    except Exception as e:
        st.error(f"‚ùå Error processing audio file: {e}")
        return None


# ‚úÖ Prediction Function
def predict_emotion(audio_file, model):
    features = extract_features(audio_file)
    if features is None:
        return None
    features = np.expand_dims(features, axis=0)
    try:
        prediction = model.predict(features)
        emotions = ['Neutral', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgust', 'Surprised']
        return emotions[np.argmax(prediction)]
    except Exception as e:
        st.error(f"‚ùå Model prediction failed: {e}")
        return None


# ‚úÖ Audio Recording Function
def record_audio(filename, duration=3, fs=22050):
    st.info("üéôÔ∏è Recording... Speak now!")
    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
    sd.wait()  # Ensure recording completes

    if np.any(recording):  # Check if recording is not silent
        wavio.write(filename, recording, fs, sampwidth=2)
        st.success(f"‚úÖ Recording saved as {filename}!")
        st.audio(filename, format='audio/wav')  # Playback recorded audio
    else:
        st.error("‚ùå Recording failed! No sound detected.")


# üé§ **UI Components**
st.title("üé§ Speech Emotion Recognition")
st.write("Upload or record an audio file to predict the emotion!")

# ‚úÖ Choose input method
option = st.radio("Choose an option:", ["üìÇ Upload Audio", "üéôÔ∏è Record Audio"])
audio_path = None

# ‚úÖ Handle file upload
if option == "üìÇ Upload Audio":
    uploaded_file = st.file_uploader("Upload a WAV file", type=["wav", "mp3"])
    if uploaded_file:
        audio_path = os.path.join(AUDIO_SAVE_PATH, "temp_upload.wav")
        with open(audio_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.success("‚úÖ Audio uploaded successfully!")
        st.audio(audio_path, format='audio/wav')

# ‚úÖ Handle recording
if option == "üéôÔ∏è Record Audio":
    if st.button("üé§ Start Recording"):
        audio_path = os.path.join(AUDIO_SAVE_PATH, "recorded_audio.wav")
        record_audio(audio_path)
        st.audio(audio_path, format='audio/wav')

# ‚úÖ Predict Emotion
if model is not None and audio_path:
    if st.button("üîç Predict Emotion"):
        try:
            emotion = predict_emotion(audio_path, model)
            if emotion:
                st.success(f"**Predicted Emotion: {emotion} üé≠**")
            else:
                st.error("‚ùå Could not predict emotion. Try a different audio file.")
        except Exception as e:
            st.error(f"‚ùå Prediction failed: {e}")

        # ‚úÖ Remove audio after prediction to avoid clutter
        os.remove(audio_path)
